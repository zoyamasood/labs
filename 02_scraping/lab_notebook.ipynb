{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Last time, we looked at REST API's as a source of data. You can get lots of very high quality data this way.\n",
    "\n",
    "Some data is available online, but not through an API. When this is the case, some times you can simply copy and paste the data into a .csv file and go on with your life. But if there are many records to parse and combine into a dataset, that might be impossible. Can we automate the collection of data from online sources?\n",
    "\n",
    "This is called web scraping. Broadly speaking: Web scraping is legal, but what you plan to do with the results of your scraping might not be. In general, most sites do not want you to scrape them at this point, but there is not really a way to stop you if you are sufficiently motivated. Be careful to use server resources respectfully (not too many requests per unit time), think seriously about privacy concerns, and be careful who you share your work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be scraping data about used cars in Charlottesville from Craigslist. This will give us a chance to put those wrangling, EDA, and visualization skills to work. \n",
    "\n",
    "We'll use the `requests` package, as we did with API's, but will be getting the kinds of web pages you see everyday. Again, we'll use a header with a user-agent that masks our true identity so that we're not rejected by the server. This particular url points to the car listings for Craigslist in Charlottesville."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import requests # Page requests\n",
    "\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0'} \n",
    "url = 'https://charlottesville.craigslist.org/search/cta?purveyor=owner#search=1~gallery~0~0' \n",
    "raw = requests.get(url,headers=header) # Get page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that particular page available locally, we want to **parse** it and get results from it. To do that, we can use a package called `beautifulSoup` or `bs4`.\n",
    "\n",
    "What does `beautifulSoup` do for us? Let's go to the web page of interest. You probably see something like this:\n",
    "\n",
    "![Listings](craigslist.png \"Craigslist\")\n",
    "\n",
    "But if you \"view page source\" -- which is CTRL+U -- in Chrome, you see what the computer sees:\n",
    "\n",
    "![Listings](craigslist_source.png \"Craigslist\")\n",
    "\n",
    "Since your web browser needs lots of instructions about how to render the text, pictures, and other content on your web page, there are a lot of clues about where the data live and how to extricate them from a page. These clues are called **tags**. If you wander the source for the search page on cars, you see a particular `class = \"cl-static-search-result\"` term appear attached to each listing: \n",
    "\n",
    "![Listings](listing.png \"Craigslist\")\n",
    "\n",
    "This structure can be exploited to search the page for information. This kind of detective work -- looking at the page source, finding the interesting tages, and then searching the page with `beautifulSoup` -- is the basic job of web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pick something else on Craigslist: Musical instruments, roommates, antiques, etc. Look at the search page and its source code. Record which fields/data you would like to gather, and what kinds of EDA you'd do with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk takes the raw content from `requests` and turns it into a beautifulSoup object, which can search the page and return results for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup # HTML parser\n",
    "bsObj = soup(raw.content,'html.parser') # Parse the html\n",
    "listings = bsObj.find_all(class_=\"cl-static-search-result\") # Find all listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the argument `class_` and not just `class`? The word `class` is a reserved keyword for Python, and cannot be used by anyone else, similar to `True` and `False`. But since we want the `class = \"cl-static-search-result\"` terms, we need to use the `class_` argument to the `.find_all` method.\n",
    "\n",
    "The `.find_all` function dredges the entire page and finds all the instances of `class = \"cl-static-search-result\"`, resulting in a list of entries. We can then parse the entries.\n",
    "\n",
    "For each listing, we'll use the `.find` method to search within the listing record for specific information. To get the information we want, we can then use `.get_text()`.\n",
    "\n",
    "In the code below, two more things happen. \n",
    "\n",
    "First, I would like to get the brand of the car from the post title, if possible. To do this, I split the title into words using `title.split()`, and then I use a list comprehension to look over every word in the title and check whether it appears in the `brands` list. \n",
    "\n",
    "Second, I would like to get the year the car was built, so I can determine the vehicle's age. To do this, I use a thing called **regular expressions** that provides a language for expressing patterns. Do I remember how to do this off the top of my head? No, I read a few pages in a book and looked on StackOverflow for answers. Roughly, in order to express the idea \"any year starting with 20xx,\" you can write `20[0-9][0-9]`, and for \"any year starting with 19xx,\" you can write `19[0-9][0-9]`. The `[0-9]`'s act as wildcards for any digit. This allows me to use the `re` package to find any instances of year-like numbers in the title text, using `re.search(r'20[0-9][0-9]|19[0-9][0-9]', title )`.\n",
    "\n",
    "This is all nested in a for-loop over the listings, and the data is appended to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regular expressions\n",
    "\n",
    "brands = ['honda', 'dodge','toyota','ford','tesla','gmc','jeep','bmw','mitsubishi','mazda',\n",
    "          'volvo','audi','volkswagen','chevy','chevrolet','acura','kia','subaru','lexus',\n",
    "          'cadillac','buick','porsche','infiniti']\n",
    "\n",
    "data = [] # We'll save our listings in this object\n",
    "for k in range( len(listings) ):\n",
    "    title = listings[k].find('div',class_='title').get_text().lower()\n",
    "    price = listings[k].find('div',class_='price').get_text()\n",
    "    link = listings[k].find(href=True)['href']\n",
    "    # Get brand from the title string:\n",
    "    words = title.split()\n",
    "    hits = [word for word in words if word in brands] # Find brands in the title\n",
    "    if len(hits) == 0:\n",
    "        brand = 'missing'\n",
    "    else:\n",
    "        brand = hits[0]\n",
    "    # Get years from title string:\n",
    "    regex_search = re.search(r'20[0-9][0-9]|19[0-9][0-9]', title ) # Find year references\n",
    "    if regex_search is None: # If no hits, record year as missing value\n",
    "        year = np.nan \n",
    "    else: # If hits, record year as first match\n",
    "        year = regex_search.group(0)\n",
    "    #\n",
    "    data.append({'title':title,'price':price,'year':year,'link':link,'brand':brand})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get your search results of interest and extract data from them, using code similar to what's above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With the data scraped from Craigslist, we can put it in a dataframe and wrangle it. Of course, price and year come in as text, not numbers, and need to be typecast/coerced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrangle the data\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df['price'] = df['price'].str.replace('$','')\n",
    "df['price'] = df['price'].str.replace(',','')\n",
    "df['price'] = pd.to_numeric(df['price'],errors='coerce')\n",
    "df['year'] = pd.to_numeric(df['year'],errors='coerce')\n",
    "df['age'] = 2025-df['year']\n",
    "print(df.shape)\n",
    "df.to_csv('craigslist_cville_cars.csv') # Save data in case of a disaster\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data in and wrangled, we can now do EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA for price and age:\n",
    "print(df['price'].describe())\n",
    "df['price'].hist(grid=False)\n",
    "plt.show()\n",
    "print(df['age'].describe())\n",
    "df['age'].hist(grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price by brand:\n",
    "df.loc[:,['price','brand']].groupby('brand').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age by brand:\n",
    "df.loc[:,['age','brand']].groupby('brand').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=df, x='age', y='price',hue='brand')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_price'] = np.log(df['price'])\n",
    "df['log_age'] = np.log(df['age'])\n",
    "\n",
    "ax = sns.scatterplot(data=df, x='log_age', y='log_price',hue='brand')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "print(df.loc[:,['log_price','log_age']].cov())\n",
    "print(df.loc[:,['log_price','log_age']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df, x='log_age', y='log_price',kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrangle your data, do some EDA, and make some plots. Try to find some interesting relationships or stories to tell about your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing we want to do is go from scraping a single page to crawling around.\n",
    "\n",
    "The idea here is that every web page is connected to some other page. By extracting links as we move from page to page, we can create a web crawler that wanders around for us, gathering information of interest.\n",
    "\n",
    "In this case, we want to use the search results to then visit each individual page for each listing. Since we saved the links to the web pages in the previous scrape, we can now simply for-loop over that column in the dataframe, visiting the page listing for each of the cars in the search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Time delays\n",
    "import random # Random numbers\n",
    "\n",
    "links = df['link']\n",
    "data = []\n",
    "for link in links: # about 3 minutes\n",
    "    time.sleep(random.randint(1, 3)) # Random delays\n",
    "    raw = requests.get(link,headers=header) # Get page\n",
    "    bsObj = soup(raw.content,'html.parser') # Parse the html\n",
    "    #\n",
    "    try:\n",
    "        year_post = bsObj.find(class_='attr important').find(class_ = 'valu year').get_text()\n",
    "    except:\n",
    "        year_post = np.nan\n",
    "    #\n",
    "    try:\n",
    "        condition = bsObj.find(class_='attr condition').find(href=True).get_text()\n",
    "    except:\n",
    "        condition = 'missing'\n",
    "    #\n",
    "    try:\n",
    "        cylinders = bsObj.find(class_='attr auto_cylinders').find(class_ = 'valu').get_text()\n",
    "        cylinders = cylinders.replace('\\n','')\n",
    "    except:\n",
    "        cylinders = 'missing'\n",
    "    #\n",
    "    try:\n",
    "        drivetrain = bsObj.find(class_='attr auto_drivetrain').find(href=True).get_text()\n",
    "    except:\n",
    "        drivetrain = 'missing'\n",
    "    #\n",
    "    try:\n",
    "        fuel = bsObj.find(class_='attr auto_fuel_type').find(href = True).get_text()\n",
    "    except:\n",
    "        fuel = 'missing'\n",
    "    #\n",
    "    try:\n",
    "        miles = bsObj.find(class_='attr auto_miles').find(class_ = 'valu').get_text()\n",
    "    except:\n",
    "        miles = np.nan\n",
    "    #\n",
    "    try:\n",
    "        color = bsObj.find(class_='attr auto_paint').find(href=True).get_text()\n",
    "    except:\n",
    "        color='missing'\n",
    "    #\n",
    "    try:\n",
    "        title = bsObj.find(class_='attr auto_title_status').find(href=True).get_text()\n",
    "    except:\n",
    "        title='missing'\n",
    "    #\n",
    "    try:\n",
    "        transmission = bsObj.find(class_='attr auto_transmission').find(href=True).get_text()\n",
    "    except:\n",
    "        transmission = 'missing'\n",
    "    #\n",
    "    try:\n",
    "        bodytype = bsObj.find(class_='attr auto_bodytype').find(href=True).get_text()\n",
    "    except:\n",
    "        bodytype = 'missing'\n",
    "    #\n",
    "    text = bsObj.find(id='postingbody').get_text()\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.replace('QR Code Link to This Post','')\n",
    "    record = {'title':title,\n",
    "              'year_post':year_post,\n",
    "              'condition':condition,\n",
    "              'cylinders':cylinders,\n",
    "              'drivetrain':drivetrain,\n",
    "              'fuel':fuel,\n",
    "              'miles':miles,\n",
    "              'color':color,\n",
    "              'title':'title',\n",
    "              'transmission':transmission,\n",
    "              'bodytype':bodytype,\n",
    "              'text':text,}\n",
    "    data.append(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the new features here?\n",
    "\n",
    "First, we don't want to overwhelm the servers, so we put a small delay between each request of a listing, `time.sleep(random.randint(1, 3))`. This waits a random amount of time between 1 and 3 seconds to avoid overwhelming their server.\n",
    "\n",
    "Second, we use the try/except block. This is a useful control structure in general, but especially for web scraping. Python tries the statements under `try:`, and if it fails, executes the steps under `except:`. This can happen, in this case, with missing data, which crashes the crawler. Instead, we put our missing codes into our dataframe right away. \n",
    "\n",
    "Third, we used `.find().find().get_text()` to find the data we're looking for. In general, the structure of mark-up langauges like HTML and XML makes it possible to \"drill down\" into their entries and extract the information of interest. This exploitation of mark-up languages could be the subject of a whole course on procuring data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data scraped, we can make a new dataframe, combine it with the old one using `pd.concat`, and do some wrangling to clean the data up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame.from_dict(data)\n",
    "new_df.head()\n",
    "\n",
    "df = pd.concat([df,new_df],axis=1) # combine data frames\n",
    "df.head()\n",
    "\n",
    "df['miles'] = df['miles'].str.replace(',','')\n",
    "df['miles'] = pd.to_numeric(df['miles'],errors='coerce')\n",
    "\n",
    "df['year_post'] = df['year_post'].str.replace(',','')\n",
    "df['year_post'] = pd.to_numeric(df['year_post'],errors='coerce')\n",
    "df.to_csv('craiglist_cville_cars_long.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From your search results, crawl to the links and extract more information about every listing in your original dataframe. Wrangle and do some EDA."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
